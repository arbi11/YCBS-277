{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gym+QTable.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOM6uHKh93QFCG26jt5ZdEb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbi11/YCBS-277/blob/master/Gym%2BQTable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQbF1yOG1Tlm",
        "colab_type": "text"
      },
      "source": [
        "# 1. Start with just typing **pip install gym** on terminal for easy install, you’ll get some classic environment to start working on your agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwnvLzhZ1MR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. It renders instance for 500 timesteps, perform random actions\n",
        "import gym\n",
        "env = gym.make('Acrobot-v1')\n",
        "env.reset()\n",
        "for _ in range(500):\n",
        "    env.render()\n",
        "    env.step(env.action_space.sample())\n",
        "# 2. To check all env available, uninstalled ones are also shown\n",
        "from gym import envs \n",
        "print(envs.registry.all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_xbCAWF1dAR",
        "colab_type": "text"
      },
      "source": [
        "# 2. You can checkout other environments like Algorithmic, Atari, Box2D and Robotics\n",
        "[AI gym](https://gym.openai.com/envs/#classic_control)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxaKz4AY1PTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make('MountainCarContinuous-v0') # try for different environements\n",
        "observation = env.reset()\n",
        "for t in range(100):\n",
        "        env.render()\n",
        "        print observation\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        print observation, reward, done, info\n",
        "        if done:\n",
        "            print(\"Finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "[Output For Mountain Car Env:] \n",
        "[-0.56252328  0.00184034]\n",
        "[-0.56081509  0.00170819] -0.00796802138459 False {}\n",
        "[Output For CartPole Env:]\n",
        "[ 0.1895078   0.55386028 -0.19064739 -1.03988221]\n",
        "[ 0.20058501  0.36171167 -0.21144503 -0.81259279] 1.0 True {}\n",
        "Finished after 52 timesteps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSWqUfeG1lTz",
        "colab_type": "text"
      },
      "source": [
        "# 3. When object interacts with environment with an action then step(…) function returns observation which represents environments state, reward a float of reward in previous action, done when its time to reset the environment or goal achieved and info a dict for debugging, it can be used for learning if it contains raw probabilities of environment’s last state. See how it works. Also, observe how observation of type Space is different for different environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twnjwtu91mYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "73feb8fc-fc89-4f5f-8c4c-b3da387c5361"
      },
      "source": [
        "import gym\n",
        "env = gym.make('MountainCarContinuous-v0') # try for different environements\n",
        "observation = env.reset()\n",
        "for t in range(100):\n",
        "        env.render()\n",
        "        print observation\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        print observation, reward, done, info\n",
        "        if done:\n",
        "            print(\"Finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "[Output For Mountain Car Env:] \n",
        "[-0.56252328  0.00184034]\n",
        "[-0.56081509  0.00170819] -0.00796802138459 False {}\n",
        "[Output For CartPole Env:]\n",
        "[ 0.1895078   0.55386028 -0.19064739 -1.03988221]\n",
        "[ 0.20058501  0.36171167 -0.21144503 -0.81259279] 1.0 True {}\n",
        "Finished after 52 timesteps"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f506f2ea960c>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print observation\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(observation)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zc_APKR1t5D",
        "colab_type": "text"
      },
      "source": [
        "# 4.  install all dependencies of gym and then completely install gym with following commands. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-ujL131ocD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "sudo pip install 'gym[all]' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WF6EVdl14I7",
        "colab_type": "text"
      },
      "source": [
        "# 5. start building a Q-table algorithm, which will try to solve FrozenLake environment[env](https://gym.openai.com/envs/FrozenLake8x8-v0/). In this environment aim is to reach the goal, on a frozen lake that might have some holes in it. Here is how surface is depicted by this algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtnpGrr_10Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np \n",
        "# 1. Load Environment and Q-table structure\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "# env.obeservation.n, env.action_space.n gives number of states and action in env loaded\n",
        "# 2. Parameters of Q-leanring\n",
        "eta = .628\n",
        "gma = .9\n",
        "epis = 5000\n",
        "rev_list = [] # rewards per episode calculate\n",
        "# 3. Q-learning Algorithm\n",
        "for i in range(epis):\n",
        "    # Reset environment\n",
        "    s = env.reset()\n",
        "    rAll = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "    #The Q-Table learning algorithm\n",
        "    while j < 99:\n",
        "        env.render()\n",
        "        j+=1\n",
        "        # Choose action from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        #Get new state & reward from environment\n",
        "        s1,r,d,_ = env.step(a)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,a] = Q[s,a] + eta*(r + gma*np.max(Q[s1,:]) - Q[s,a])\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        if d == True:\n",
        "            break\n",
        "    rev_list.append(rAll)\n",
        "    env.render()\n",
        "print \"Reward Sum on all episodes \" + str(sum(rev_list)/epis)\n",
        "print \"Final Values Q-Table\"\n",
        "print Q"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
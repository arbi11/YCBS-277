{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YCBS_277_PG.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdxsDzF1Dc6wpyePPuUJyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbi11/YCBS-277/blob/master/YCBS_277_PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wBH6TCa5_cV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c37c86ed-619d-46e8-a6cd-1fa6f5e725c7"
      },
      "source": [
        "\"\"\"\n",
        "Simple policy gradient in Keras\n",
        "From Keras documentation\n",
        "\n",
        "\"\"\"\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import utils as np_utils\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
        "        \"\"\"Gym Playing Agent\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): the dimension of state.\n",
        "                Same as `env.observation_space.shape[0]`\n",
        "\n",
        "            output_dim (int): the number of discrete actions\n",
        "                Same as `env.action_space.n`\n",
        "\n",
        "            hidden_dims (list): hidden dimensions\n",
        "\n",
        "        Methods:\n",
        "\n",
        "            private:\n",
        "                __build_train_fn -> None\n",
        "                    It creates a train function\n",
        "                    It's similar to defining `train_op` in Tensorflow\n",
        "                __build_network -> None\n",
        "                    It create a base model\n",
        "                    Its output is each action probability\n",
        "\n",
        "            public:\n",
        "                get_action(state) -> action\n",
        "                fit(state, action, reward) -> None\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.__build_network(input_dim, output_dim, hidden_dims)\n",
        "        self.__build_train_fn()\n",
        "\n",
        "    def __build_network(self, input_dim, output_dim, hidden_dims=[32, 32]):\n",
        "        \"\"\"Create a base network\"\"\"\n",
        "        self.X = layers.Input(shape=(input_dim,))\n",
        "        net = self.X\n",
        "\n",
        "        for h_dim in hidden_dims:\n",
        "            net = layers.Dense(h_dim)(net)\n",
        "            net = layers.Activation(\"relu\")(net)\n",
        "\n",
        "        net = layers.Dense(output_dim)(net)\n",
        "        net = layers.Activation(\"softmax\")(net)\n",
        "\n",
        "        self.model = Model(inputs=self.X, outputs=net)\n",
        "\n",
        "    def __build_train_fn(self):\n",
        "        \"\"\"Create a train function\n",
        "\n",
        "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.\n",
        "\n",
        "        For example, we need action placeholder\n",
        "        called `action_one_hot` that stores, which action we took at state `s`.\n",
        "        Hence, we can update the same action.\n",
        "\n",
        "        This function will create\n",
        "        `self.train_fn([state, action_one_hot, discount_reward])`\n",
        "        which would train the model.\n",
        "\n",
        "        \"\"\"\n",
        "        action_prob_placeholder = self.model.output\n",
        "        action_onehot_placeholder = K.placeholder(shape=(None, self.output_dim),\n",
        "                                                  name=\"action_onehot\")\n",
        "        discount_reward_placeholder = K.placeholder(shape=(None,),\n",
        "                                                    name=\"discount_reward\")\n",
        "\n",
        "        action_prob = K.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
        "        log_action_prob = K.log(action_prob)\n",
        "\n",
        "        loss = - log_action_prob * discount_reward_placeholder\n",
        "        loss = K.mean(loss)\n",
        "\n",
        "        adam = optimizers.Adam()\n",
        "\n",
        "        updates = adam.get_updates(params=self.model.trainable_weights,\n",
        "                                   loss=loss)\n",
        "\n",
        "        self.train_fn = K.function(inputs=[self.model.input,\n",
        "                                           action_onehot_placeholder,\n",
        "                                           discount_reward_placeholder],\n",
        "                                   outputs=[],\n",
        "                                   updates=updates)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Returns an action at given `state`\n",
        "\n",
        "        Args:\n",
        "            state (1-D or 2-D Array): It can be either 1-D array of shape (state_dimension, )\n",
        "                or 2-D array shape of (n_samples, state_dimension)\n",
        "\n",
        "        Returns:\n",
        "            action: an integer action value ranging from 0 to (n_actions - 1)\n",
        "        \"\"\"\n",
        "        shape = state.shape\n",
        "\n",
        "        if len(shape) == 1:\n",
        "            assert shape == (self.input_dim,), \"{} != {}\".format(shape, self.input_dim)\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "\n",
        "        elif len(shape) == 2:\n",
        "            assert shape[1] == (self.input_dim), \"{} != {}\".format(shape, self.input_dim)\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Wrong state shape is given: {}\".format(state.shape))\n",
        "\n",
        "        action_prob = np.squeeze(self.model.predict(state))\n",
        "        assert len(action_prob) == self.output_dim, \"{} != {}\".format(len(action_prob), self.output_dim)\n",
        "        return np.random.choice(np.arange(self.output_dim), p=action_prob)\n",
        "\n",
        "    def fit(self, S, A, R):\n",
        "        \"\"\"Train a network\n",
        "\n",
        "        Args:\n",
        "            S (2-D Array): `state` array of shape (n_samples, state_dimension)\n",
        "            A (1-D Array): `action` array of shape (n_samples,)\n",
        "                It's simply a list of int that stores which actions the agent chose\n",
        "            R (1-D Array): `reward` array of shape (n_samples,)\n",
        "                A reward is given after each action.\n",
        "\n",
        "        \"\"\"\n",
        "        action_onehot = np_utils.to_categorical(A, num_classes=self.output_dim)\n",
        "        discount_reward = compute_discounted_R(R)\n",
        "\n",
        "        assert S.shape[1] == self.input_dim, \"{} != {}\".format(S.shape[1], self.input_dim)\n",
        "        assert action_onehot.shape[0] == S.shape[0], \"{} != {}\".format(action_onehot.shape[0], S.shape[0])\n",
        "        assert action_onehot.shape[1] == self.output_dim, \"{} != {}\".format(action_onehot.shape[1], self.output_dim)\n",
        "        assert len(discount_reward.shape) == 1, \"{} != 1\".format(len(discount_reward.shape))\n",
        "\n",
        "        self.train_fn([S, action_onehot, discount_reward])\n",
        "\n",
        "\n",
        "def compute_discounted_R(R, discount_rate=.99):\n",
        "    \"\"\"Returns discounted rewards\n",
        "\n",
        "    Args:\n",
        "        R (1-D array): a list of `reward` at each time step\n",
        "        discount_rate (float): Will discount the future value by this rate\n",
        "\n",
        "    Returns:\n",
        "        discounted_r (1-D array): same shape as input `R`\n",
        "            but the values are discounted\n",
        "\n",
        "    Examples:\n",
        "        >>> R = [1, 1, 1]\n",
        "        >>> compute_discounted_R(R, .99) # before normalization\n",
        "        [1 + 0.99 + 0.99**2, 1 + 0.99, 1]\n",
        "    \"\"\"\n",
        "    discounted_r = np.zeros_like(R, dtype=np.float32)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(R))):\n",
        "\n",
        "        running_add = running_add * discount_rate + R[t]\n",
        "        discounted_r[t] = running_add\n",
        "\n",
        "    discounted_r -= discounted_r.mean() / discounted_r.std()\n",
        "\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def run_episode(env, agent):\n",
        "    \"\"\"Returns an episode reward\n",
        "\n",
        "    (1) Play until the game is done\n",
        "    (2) The agent will choose an action according to the policy\n",
        "    (3) When it's done, it will train from the game play\n",
        "\n",
        "    Args:\n",
        "        env (gym.env): Gym environment\n",
        "        agent (Agent): Game Playing Agent\n",
        "\n",
        "    Returns:\n",
        "        total_reward (int): total reward earned during the whole episode\n",
        "    \"\"\"\n",
        "    done = False\n",
        "    S = []\n",
        "    A = []\n",
        "    R = []\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        s2, r, done, info = env.step(a)\n",
        "        total_reward += r\n",
        "\n",
        "        S.append(s)\n",
        "        A.append(a)\n",
        "        R.append(r)\n",
        "\n",
        "        s = s2\n",
        "\n",
        "        if done:\n",
        "            S = np.array(S)\n",
        "            A = np.array(A)\n",
        "            R = np.array(R)\n",
        "\n",
        "            agent.fit(S, A, R)\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        env = gym.make(\"CartPole-v0\")\n",
        "        input_dim = env.observation_space.shape[0]\n",
        "        output_dim = env.action_space.n\n",
        "        agent = Agent(input_dim, output_dim, [16, 16])\n",
        "\n",
        "        for episode in range(2000):\n",
        "            reward = run_episode(env, agent)\n",
        "            print(episode, reward)\n",
        "\n",
        "    finally:\n",
        "        env.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1017 200.0\n",
            "1018 200.0\n",
            "1019 200.0\n",
            "1020 200.0\n",
            "1021 200.0\n",
            "1022 200.0\n",
            "1023 200.0\n",
            "1024 200.0\n",
            "1025 200.0\n",
            "1026 200.0\n",
            "1027 200.0\n",
            "1028 200.0\n",
            "1029 200.0\n",
            "1030 200.0\n",
            "1031 200.0\n",
            "1032 200.0\n",
            "1033 200.0\n",
            "1034 200.0\n",
            "1035 200.0\n",
            "1036 200.0\n",
            "1037 200.0\n",
            "1038 200.0\n",
            "1039 200.0\n",
            "1040 200.0\n",
            "1041 200.0\n",
            "1042 200.0\n",
            "1043 200.0\n",
            "1044 200.0\n",
            "1045 200.0\n",
            "1046 200.0\n",
            "1047 200.0\n",
            "1048 200.0\n",
            "1049 200.0\n",
            "1050 200.0\n",
            "1051 200.0\n",
            "1052 200.0\n",
            "1053 200.0\n",
            "1054 200.0\n",
            "1055 200.0\n",
            "1056 200.0\n",
            "1057 200.0\n",
            "1058 200.0\n",
            "1059 200.0\n",
            "1060 200.0\n",
            "1061 200.0\n",
            "1062 200.0\n",
            "1063 200.0\n",
            "1064 200.0\n",
            "1065 200.0\n",
            "1066 200.0\n",
            "1067 200.0\n",
            "1068 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d7288309dd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-d7288309dd3a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d7288309dd3a>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d7288309dd3a>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong state shape is given: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{} != {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxEAZ7ut7QxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
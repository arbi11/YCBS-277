{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCTS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbkgqn/Xuk36fytNXJct6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbi11/YCBS-277/blob/master/MCTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ftvlyMf0tQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Create an environment\n",
        "\n",
        "env = WormSRMEnv(env_dim = constants_SRM.env_dim, startR=1 , startC= 1, max_steps = max_steps)\n",
        "state = env.reset()\n",
        "\n",
        "txt_writer = append_txt_file   \n",
        "\n",
        "qstates[0] = state\n",
        "qdict[str(0)] = np.zeros([4])\n",
        "qcount[str(0)] = np.zeros([4])\n",
        "#np.where((qstates==state[:,None]).all(-1))[1]\n",
        "\n",
        "#dims = qstates.max(0)+1\n",
        "#X1D = np.ravel_multi_index(qstates.T,dims)\n",
        "\n",
        "#out = np.where(np.in1d(np.ravel_multi_index(qstates.T,dims),\\\n",
        "#                    np.ravel_multi_index(state.T,dims)))[0]\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()         \n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    net_force = 0.0\n",
        "    exploration = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        search_state = np.expand_dims(state, axis= 0)            \n",
        "        a = np.where((qstates==search_state[:,None]).all(-1))[1]\n",
        "        if len(a) == 1:\n",
        "            index = a[0]\n",
        "            s_index = str(index)\n",
        "            \n",
        "        elif len(a) == 0:\n",
        "            qstates = np.append(qstates, search_state, axis=0)\n",
        "            a = np.where((qstates==search_state[:,None]).all(-1))[1]\n",
        "            s_index = str(a[0])\n",
        "            qdict[s_index] = np.zeros([4])\n",
        "            qcount[s_index] = np.zeros([4])\n",
        "#            print('Adding new states')\n",
        "                \n",
        "        else:\n",
        "            print('Error in the system')\n",
        "            \t\t\t            \n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state) Greedy\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qdict[s_index][:])\n",
        "\n",
        "        # Else doing a random choice --> exploration based on probablity of the q-values of the actions in a state\n",
        "        else:\n",
        "#            action = env.action_space.sample()\n",
        "            action = np.random.choice(np.arange(0,4), p=normalize(qdict[s_index][:]))\n",
        "#            print('action:', action)\n",
        "            exploration += 1\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Updating Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        search_new_state = np.expand_dims(new_state, axis= 0)            \n",
        "        a_new = np.where((qstates == search_new_state[:,None]).all(-1))[1]\n",
        "        if len(a_new) == 1:\n",
        "            index_new = a_new[0]\n",
        "            s_index_new = str(index_new)\n",
        "\n",
        "        elif len(a_new) == 0:\n",
        "            qstates = np.append(qstates, search_new_state, axis=0)\n",
        "            a_new = np.where((qstates==search_new_state[:,None]).all(-1))[1]\n",
        "            s_index_new = str(a_new[0])\n",
        "            qdict[s_index_new] = np.zeros([4])\n",
        "            qcount[s_index_new] = np.zeros([4])\n",
        "#            print('Adding new states')\n",
        "    \n",
        "        else:\n",
        "            print('Error in the system')\n",
        "        \n",
        "        qdict[s_index][action] = qdict[s_index][action]*qcount[s_index][action] + learning_rate * (reward + gamma * np.max(qdict[s_index_new][:]) - qdict[s_index][action])       \n",
        "\n",
        "#qtable[state, action] = qtable[state, action]*qcount[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        qcount[s_index][action] += 1\n",
        "        qdict[s_index][action] = qdict[s_index][action]/qcount[s_index][action]\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (iron count = 200 or steps = max_steps)\n",
        "        if done == True:\n",
        "            net_force = env.netT\n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    \n",
        "    txt_content = ('Epoch {:d}, Reward {:.3f}, iron_c: {:d}, Exp {:d}, steps: {}, Net Torque: {:.3f}'\n",
        "\t\t\t\t            .format(episode, total_rewards, env.count, exploration, env.step_count, net_force))\n",
        "    txt_writer('\\n' + txt_content)        \n",
        "    print(txt_content)\n",
        "   \n",
        "    rewards.append(total_rewards)\n",
        "    if episode % play_time == 0:\n",
        "        print('Playing the game after the {:2.0f}th epoch:--->'.format((episode)))\n",
        "        \n",
        "#        env.save_checkpoints(qstates, '\\\\qstates')         \n",
        "#        env.save_checkpoints(qdict, '\\\\qdict')    \n",
        "#        env.save_checkpoints(qcount, '\\\\qcount')         \n",
        "        done = False\n",
        "         \n",
        "        state = env.reset()\n",
        "        episodic_reward = 0.0\n",
        "        net_force = 0.0\n",
        "        while not done:\n",
        "            \n",
        "            env.render(train= False)\n",
        "            search_state = np.expand_dims(state, axis= 0)\n",
        "            a = np.where((qstates==search_state[:,None]).all(-1))[1]\n",
        "            if len(a) == 1:      # If the state has already been visited\n",
        "                index = a[0]\n",
        "                s_index = str(index)\n",
        "                \n",
        "            elif len(a) == 0:    # First visit to a state\n",
        "                qstates = np.append(qstates, search_state, axis=0)\n",
        "                a = np.where((qstates==search_state[:,None]).all(-1))[1]\n",
        "                s_index = str(a[0])\n",
        "                qdict[s_index] = np.zeros([4])\n",
        "                qcount[s_index] = np.zeros([4])               \n",
        "                \n",
        "            else:     # probably a state has been stored twice\n",
        "                print('Error in the system')                   \n",
        "                   \n",
        "            action = np.argmax(qdict[s_index][:])\n",
        "#            action = np.argmax(actions, axis= 1)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            state = new_state\n",
        "            episodic_reward += reward\n",
        "            if done == True:\n",
        "                net_force = env.netT\n",
        "        \n",
        "        test_txt_content = ('\\t Episodic Reward: {:2.4f}, iron_c: {:d}, Net Torque: {:2.4f}'\\\n",
        "                     .format(episodic_reward, env.count, net_force))\n",
        "\n",
        "        txt_writer('\\n' + test_txt_content)                      \n",
        "        print(test_txt_content)                                        \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}